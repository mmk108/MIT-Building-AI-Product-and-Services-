{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks in Python\n",
    "\n",
    "**Jessica Cervi**\n",
    "\n",
    "## Activity Overview \n",
    "\n",
    "The term deep neural networks (DNNs) broadly refers to any kind of neural network with many layers, assembled in order to achieve some larger task.\n",
    "\n",
    "In this activity, we'll first explore how to assemble a simplified version of a DNN using the familiar MNIST digits dataset. We will train our network and measure its accuracy to see if it makes a correct prediction.\n",
    "\n",
    "This activity is designed to help you apply the machine learning algorithms you have learned using the packages in `Python`. `Python` concepts, instructions, and starter code are embedded within this Jupyter Notebook to help guide you as you progress through the activity. Remember to run the code of each code cell prior to submitting the assignment. Upon completing the activity, we encourage you to compare your work against the solution file to perform a self-assessment.\n",
    "\n",
    "## Index:\n",
    "\n",
    "#### Week 3:  Deep Neural Networks\n",
    "\n",
    "- [Part 1](#part1) - Deep Neural Networks\n",
    "- [Part 2](#part2) - Setting up the Problem\n",
    "- [Part 3](#part3) - Creating the Model\n",
    "- [Part 4](#part4) - Compiling the Model \n",
    "- [Part 5](#part5) - Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part1'></a>\n",
    "\n",
    "# Deep Neural Networks <a></a>\n",
    "\n",
    "Deep learning is becoming a very popular subset of machine learning due to its high level of performance across many types of data. One of the most typical uses of deep learning to classify images is to build a deep neural network (DNN). \n",
    "\n",
    "Computers see images using pixels. Pixels in images are usually related, particularly to other nearby pixels. For example, a certain group of pixels may signify an edge in an image, a particular texture, or some other pattern. Convolutions use this to help identify and classify these images.\n",
    "\n",
    "For example, to achieve image classification using DNNs, we may use a sequence of convolution, ReLU and pooling layers, whose purpose is to essentially learn and extract relevant features from the image. \n",
    "\n",
    "That might then be combined with a flatten operation and a sequence of fully connected layers, culminating in some output, for example, using a softmax activation function at the end to indicate the “best” choice among several possible classes for the image.\n",
    "\n",
    "In this activity, we will use a simplified DNN consisting  of only three layers: a convolution, a flattening layer, and a softmax activation. Details about each of these layes will be given in the next sections of this activity.\n",
    "\n",
    "\n",
    "<img src=\"steps.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers in Python with Keras\n",
    "\n",
    "Since we are going to work with images,  it's a good idea to use a DNN.\n",
    "In a similar way as we did for the coding activity about autoencoders, we will use the Python library [Keras](https://keras.io) to set up and build our problem.\n",
    "\n",
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part2'></a>\n",
    "\n",
    "## Setting up the Problem <a></a>\n",
    "\n",
    "Again. we will be using the [`MNIST`](https://en.wikipedia.org/wiki/MNIST_database) database for our model.\n",
    "The MNIST database contains images of handwritten digits 0 through 9  normalized to fit into a 28x28 pixel bounding box.\n",
    "The MNIST dataset can be conveniently imported through `Keras`. \n",
    "\n",
    "Run the code cell below to import some of the libraries we will use in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 4s 0us/step\n",
      "11501568/11490434 [==============================] - 4s 0us/step\n",
      "(60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s take a look at one of the images in our dataset to see what we're working with. We will plot the first image in our dataset using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plot the first image in the dataset\n",
    "plt.imshow(X_train[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to check the size of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check image shape\n",
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the shape of every image in the MNIST dataset is 28x28, so we will not need to check the shape of all the images. When using real-world datasets, you may not be so lucky. 28x28 is also a fairly small size, so the DNN will be able to run over each image pretty quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "The first step in this activity is to prepare the data before feeding it to our DNN.\n",
    "We will do this by reshaping each image from (28, 28) to (28, 28, 1) because `Keras` requires the third dimension. Note that this structure is sometimes referred to as a \"tensor\" in which there may be multiple \"depth\" channels; this terminology gives us some insight into the naming of the `TensorFlow` tool that `Keras` provides an interface to.\n",
    "\n",
    "Run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#reshape data to fit model\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, fill in the ellipsis to reshape the `X_test` set using the same dimensions we have used for `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(10000,28,28,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step of our data preparation, we need to ‘one-hot-encode’ our target variable. To achieve this, we will be using the function [`to_categorical`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) from the `Keras` module `utils`.\n",
    "\n",
    "This means that a column will be created for each output category and a separate binary variable is created for each category. For example, we saw that the first image in the dataset is a 5. This means that the sixth number in our array (corresponding to the sixth possible digit type counting from 0) will have a 1 and the rest of the array will be filled with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train[0]: 5\n",
      "y_train_onehot[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "#one-hot encode target column\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)\n",
    "print(\"y_train[0]:\", y_train[0])\n",
    "print(\"y_train_onehot[0]:\", y_train_onehot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part3'></a>\n",
    "\n",
    "## Creating the Model <a></a>\n",
    "\n",
    "Now we are ready to build our model.\n",
    "\n",
    "The model type that we will be using is Sequential. The `Keras` class [`sequential`](https://keras.io/api/models/sequential/) is the easiest way to build a model in Keras. It allows you to build a model layer by layer.\n",
    "\n",
    "In the code below, fill in the ellipsis so that `num_filters`, `filter_size`, and `pool_size` are equal to 8, 3, and 2, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "#set up parameters for the model\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `add()` function to add layers to our model.\n",
    "\n",
    "Our first layer is a Conv2D layer. This is a convolution layer that will deal with our input images, which are seen as 2-dimensional matrices. In the code cell below, 8 is the number of nodes in each layer. This number can be adjusted to be higher or lower, depending on the size of the dataset. Our first layer also takes in the shape of an input. The shape of each input image is 28,28,1, as seen earlier, with the 1 signifying that the images are greyscale.\n",
    "\n",
    "Next, we add a pooling layer by using the function [`MaxPooling2D`](https://keras.io/api/layers/pooling_layers/max_pooling2d/). Pooling is usually employed as an important strategy to achieve high performance in image classification. It works by taking the maximum value in a region around each pixel, as given by the `pool_size` variable. \n",
    "\n",
    "In between the `Conv2D` layers and the dense layer, there is a [`Flatten`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) layer. Flatten serves as a connection between the convolution and dense layers, converting higher-dimensional data into a single 1-dimensional vector as needed by a dense layer.\n",
    "\n",
    "Finally, as we have seen for autoencoders, `Dense` is the layer type that we will use for our output layer. Dense is a standard layer type that is used in many cases for neural networks.\n",
    "\n",
    "We will have 10 nodes in our output layer, one for each possible outcome (0–9).\n",
    "The activation is \"softmax\". Softmax makes the output sum up to 1 so the output can be interpreted as probabilities. The model will then make its prediction based on whichever option or class has the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model\n",
    "model = Sequential()\n",
    "\n",
    "#add model layers\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part4'></a>\n",
    "\n",
    "## Compiling the Model <a></a>\n",
    "\n",
    "Next, we need to compile our model. Compiling the model takes three parameters: optimizer, loss, and metrics.\n",
    "\n",
    "The optimizer controls the learning rate. We will be using `adam` as our optmizer. Adam is generally a good optimizer to use for many cases. The adam optimizer adjusts the learning rate throughout training.\n",
    "The learning rate determines how fast the optimal weights for the model are calculated. A smaller learning rate may lead to more accurate weights (up to a certain point), but as we saw, the time it takes to compute the weights will be longer.\n",
    "\n",
    "We will use `'categorical_crossentropy'` for our loss function. This is the most common choice for classification. A lower score indicates that the model is performing better.\n",
    "To make things even easier to interpret, we will use the `'accuracy'` metric to see the accuracy score on the validation set when we train the model.\n",
    "\n",
    "In the code cell, fill in the ellipsis to set the argument `loss` equal to `'categorical_crossentropy'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part5'></a>\n",
    "\n",
    "## Training the Model <a></a>\n",
    "\n",
    "\n",
    "Now, we will train our model. \n",
    "\n",
    "We will train the data in a similar way as we did for autoencoders.\n",
    "\n",
    "\n",
    "For our validation data, we will use the test set provided to us in our dataset, which we have processed into `X_test` and `y_test_onehot`.\n",
    "The number of epochs is the number of times the model will cycle through the data. The more epochs we run, the more the model will improve up to a certain point. After that point, the model will stop improving during each epoch. \n",
    "\n",
    "For efficiency, in our model below we will set the number of epochs to 3.\n",
    "\n",
    "Run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 2.3873 - accuracy: 0.8917 - val_loss: 0.7592 - val_accuracy: 0.9211\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.3932 - accuracy: 0.9414 - val_loss: 0.3415 - val_accuracy: 0.9407\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.2449 - accuracy: 0.9499 - val_loss: 0.2546 - val_accuracy: 0.9517\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(X_train, y_train_onehot, validation_data=(X_test, y_test_onehot), epochs=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "What is the accuracy of our model after 3 epochs? Round your answer to two decimal digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95.17%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "If you want to see the actual predictions that our model has made for the test data, we can use the [`predict` ](https://www.tensorflow.org/api_docs/python/tf/keras/Model) function. \n",
    "\n",
    "The predict function will give an array with 10 numbers. \n",
    "Note that these numbers are the probabilities that the input image represents each respective digit (0–9). The array index with the highest number represents the model prediction. The sum of each array equals 1 (since each number is a probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.6365697e-18, 3.1769081e-19, 2.0355354e-10, 5.5570726e-10,\n",
       "        4.2699961e-14, 4.2122250e-17, 3.2271563e-28, 1.0000000e+00,\n",
       "        8.6690068e-14, 9.8199719e-11],\n",
       "       [2.4969959e-12, 7.5684788e-12, 1.0000000e+00, 1.5889869e-11,\n",
       "        9.5199150e-18, 6.7047741e-09, 7.1094917e-09, 2.2752326e-23,\n",
       "        3.7693931e-11, 6.5715597e-23],\n",
       "       [1.9788970e-09, 9.9990594e-01, 5.1786548e-05, 3.4336985e-08,\n",
       "        9.7874363e-06, 6.3000917e-11, 7.0775572e-09, 2.4565705e-07,\n",
       "        3.2204498e-05, 6.1342460e-09],\n",
       "       [1.0000000e+00, 3.1573946e-20, 8.7555969e-11, 6.2763884e-17,\n",
       "        1.0455632e-15, 4.0654887e-16, 5.3483110e-11, 3.4695543e-15,\n",
       "        1.2488960e-14, 1.4896205e-15]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict all images in the test set, and look at first 4\n",
    "pred_probs = model.predict(X_test)\n",
    "pred_probs[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "What is the predicted digit type of the last image shown above?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLICK ON THIS CELL TO TYPE YOUR ANSWER**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s compare this with the actual results. It's a little easier if we first convert our prediction probabilities into the corresponding class label. We can use `argmax` for that, to find the label with the maximum probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_classes = np.argmax(pred_probs,axis=1)\n",
    "pred_classes[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#actual results for first 4 images in test set\n",
    "y_test[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, our model predicted the labels of the first four images correctly!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
